{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3294209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.26200.6901]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(aml-sandbox-env) q:\\aml\\tutorial-fine-tune\\LLM-Finetuning\\AzureML\\1.SFT-LoRA>az login --use-device-code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code DC8YC37GD to authenticate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"homeTenantId\": \"72d2d42e-2b8a-4fa9-9e49-59eb283dc03f\",\n",
      "    \"id\": \"ec782a34-0145-4786-b0e2-fc16ff1da279\",\n",
      "    \"isDefault\": true,\n",
      "    \"managedByTenants\": [],\n",
      "    \"name\": \"Visual Studio Enterprise Subscription\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantDefaultDomain\": \"valitovroutlook.onmicrosoft.com\",\n",
      "    \"tenantDisplayName\": \"Default Directory\",\n",
      "    \"tenantId\": \"72d2d42e-2b8a-4fa9-9e49-59eb283dc03f\",\n",
      "    \"user\": {\n",
      "      \"name\": \"valitovr@outlook.com\",\n",
      "      \"type\": \"user\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "\n",
      "(aml-sandbox-env) q:\\aml\\tutorial-fine-tune\\LLM-Finetuning\\AzureML\\1.SFT-LoRA>"
     ]
    }
   ],
   "source": [
    "%%script cmd\n",
    "az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb1b860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"environmentName\": \"AzureCloud\",\n",
      "  \"homeTenantId\": \"72d2d42e-2b8a-4fa9-9e49-59eb283dc03f\",\n",
      "  \"id\": \"ec782a34-0145-4786-b0e2-fc16ff1da279\",\n",
      "  \"isDefault\": true,\n",
      "  \"managedByTenants\": [],\n",
      "  \"name\": \"Visual Studio Enterprise Subscription\",\n",
      "  \"state\": \"Enabled\",\n",
      "  \"tenantDefaultDomain\": \"valitovroutlook.onmicrosoft.com\",\n",
      "  \"tenantDisplayName\": \"Default Directory\",\n",
      "  \"tenantId\": \"72d2d42e-2b8a-4fa9-9e49-59eb283dc03f\",\n",
      "  \"user\": {\n",
      "    \"name\": \"valitovr@outlook.com\",\n",
      "    \"type\": \"user\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edff0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import logging\n",
    "import yaml\n",
    "from azure.ai.ml import MLClient, Input, Output, command\n",
    "from azure.ai.ml.entities import Environment, Model\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42116e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# read the Azure ML workspace configuration from config.yml\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Azure ML workspace configuration\n",
    "subscription_id = config[\"subscription_id\"]\n",
    "resource_group = config[\"resource_group\"]\n",
    "workspace_name = config[\"workspace_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c96a5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_ml_client = MLClient(DefaultAzureCredential(), registry_name=\"azureml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e450c5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/latest/pipelineComponent.schema.json\n",
      "name: chat_completion_pipeline\n",
      "version: 0.0.80\n",
      "display_name: Chat Completion Pipeline\n",
      "description: Pipeline Component to finetune Hugging Face pretrained models for chat\n",
      "  completion task. The component supports optimizations such as LoRA, Deepspeed and\n",
      "  ONNXRuntime for performance enhancement. See [docs](https://aka.ms/azureml/components/chat_completion_pipeline)\n",
      "  to learn more.\n",
      "type: pipeline\n",
      "inputs:\n",
      "  pytorch_model_path:\n",
      "    type: custom_model\n",
      "    description: Pytorch model asset path. Special characters like \\ and ' are invalid\n",
      "      in the parameter value.\n",
      "    optional: true\n",
      "  mlflow_model_path:\n",
      "    type: mlflow_model\n",
      "    description: MLflow model asset path. Special characters like \\ and ' are invalid\n",
      "      in the parameter value.\n",
      "    optional: true\n",
      "  train_file_path:\n",
      "    type: uri_file\n",
      "    description: Path to the registered training data asset. The supported data formats\n",
      "      are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \\ and\n",
      "      ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  validation_file_path:\n",
      "    type: uri_file\n",
      "    description: Path to the registered validation data asset. The supported data\n",
      "      formats are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters\n",
      "      like \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  test_file_path:\n",
      "    type: uri_file\n",
      "    description: Path to the registered test data asset. The supported data formats\n",
      "      are `jsonl`, `json`, `csv`, `tsv` and `parquet`. Special characters like \\ and\n",
      "      ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  train_mltable_path:\n",
      "    type: mltable\n",
      "    description: Path to the registered training data asset in `mltable` format. Special\n",
      "      characters like \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  validation_mltable_path:\n",
      "    type: mltable\n",
      "    description: Path to the registered validation data asset in `mltable` format.\n",
      "      Special characters like \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  test_mltable_path:\n",
      "    type: mltable\n",
      "    description: Path to the registered test data asset in `mltable` format. Special\n",
      "      characters like \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  deepspeed:\n",
      "    type: uri_file\n",
      "    description: Deepspeed config to be used for finetuning. Special characters like\n",
      "      \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  evaluation_config:\n",
      "    type: uri_file\n",
      "    description: Additional parameters for Computing Metrics. Special characters like\n",
      "      \\ and ' are invalid in the parameter value.\n",
      "    optional: true\n",
      "  instance_type_model_import:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: Standard_d12_v2\n",
      "    description: Instance type to be used for model_import component in case of serverless\n",
      "      compute, eg. standard_d12_v2. The parameter compute_model_import must be set\n",
      "      to 'serverless' for instance_type to be used\n",
      "  instance_type_preprocess:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: Standard_d12_v2\n",
      "    description: Instance type to be used for preprocess component in case of serverless\n",
      "      compute, eg. standard_d12_v2. The parameter compute_preprocess must be set to\n",
      "      'serverless' for instance_type to be used\n",
      "  instance_type_finetune:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: Standard_nc24rs_v3\n",
      "    description: Instance type to be used for finetune component in case of serverless\n",
      "      compute, eg. standard_nc24rs_v3. The parameter compute_finetune must be set\n",
      "      to 'serverless' for instance_type to be used\n",
      "  instance_type_model_evaluation:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: Standard_nc24rs_v3\n",
      "    description: Instance type to be used for model_evaluation components in case\n",
      "      of serverless compute, eg. standard_nc24rs_v3. The parameter compute_model_evaluation\n",
      "      must be set to 'serverless' for instance_type to be used\n",
      "  shm_size_finetune:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 5g\n",
      "    description: Shared memory size to be used for finetune component. It is useful\n",
      "      while using Nebula (via DeepSpeed) which uses shared memory to save model and\n",
      "      optimizer states.\n",
      "  num_nodes_finetune:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: number of nodes to be used for finetuning (used for distributed training)\n",
      "    min: '1'\n",
      "  number_of_gpu_to_use_finetuning:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: number of gpus to be used per node for finetuning, should be equal\n",
      "      to number of gpu per node in the compute SKU used for finetune\n",
      "    min: '1'\n",
      "  huggingface_id:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: The string can be any valid Hugging Face id from the [Hugging Face\n",
      "      models webpage](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads).\n",
      "      Models from Hugging Face are subject to third party license terms available\n",
      "      on the Hugging Face model details page. It is your responsibility to comply\n",
      "      with the model's license terms.\n",
      "  task_name:\n",
      "    type: string\n",
      "    optional: false\n",
      "    default: ChatCompletion\n",
      "    description: ChatCompletion task type\n",
      "    enum:\n",
      "    - ChatCompletion\n",
      "  batch_size:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1000'\n",
      "    description: Number of examples to batch before calling the tokenization function\n",
      "    min: '1'\n",
      "  pad_to_max_length:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to True, the returned sequences will be padded according to\n",
      "      the model's padding side and padding index, up to their `max_seq_length`. If\n",
      "      no `max_seq_length` is specified, the padding is done up to the model's max\n",
      "      length.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  max_seq_length:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '-1'\n",
      "    description: Controls the maximum length to use when pad_to_max_length parameter\n",
      "      is set to `true`. Default is -1 which means the padding is done up to the model's\n",
      "      max length. Else will be padded to `max_seq_length`.\n",
      "  apply_lora:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If \"true\" enables lora.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  merge_lora_weights:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'true'\n",
      "    description: If \"true\", the lora weights are merged with the base Hugging Face\n",
      "      model weights before saving.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  lora_alpha:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '128'\n",
      "    description: alpha attention parameter for lora.\n",
      "  lora_r:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '8'\n",
      "    description: lora dimension\n",
      "  lora_dropout:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.0'\n",
      "    description: lora dropout value\n",
      "  num_train_epochs:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: Number of epochs to run for finetune.\n",
      "    min: '1'\n",
      "  max_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '-1'\n",
      "    description: If set to a positive number, the total number of training steps to\n",
      "      perform. Overrides 'epochs'. In case of using a finite iterable dataset the\n",
      "      training may stop before reaching the set number of steps when all data is exhausted.\n",
      "  per_device_train_batch_size:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: Per gpu batch size used for training. The effective training batch\n",
      "      size is _per_device_train_batch_size_ * _num_gpus_ * _num_nodes_.\n",
      "    min: '1'\n",
      "  per_device_eval_batch_size:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: Per gpu batch size used for validation. The default value is 1. The\n",
      "      effective validation batch size is _per_device_eval_batch_size_ * _num_gpus_\n",
      "      * _num_nodes_.\n",
      "    min: '1'\n",
      "  auto_find_batch_size:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to \"true\" and if the provided 'per_device_train_batch_size'\n",
      "      goes into Out Of Memory (OOM) auto_find_batch_size will find the correct batch\n",
      "      size by iteratively reducing batch size by a factor of 2 till the OOM is fixed\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  optim:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: adamw_torch\n",
      "    description: Optimizer to be used while training\n",
      "    enum:\n",
      "    - adamw_torch\n",
      "    - adafactor\n",
      "  learning_rate:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: 2e-05\n",
      "    description: Start learning rate used for training.\n",
      "  warmup_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '0'\n",
      "    description: Number of steps for the learning rate scheduler warmup phase.\n",
      "  weight_decay:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.0'\n",
      "    description: Weight decay to apply (if not zero) to all layers except all bias\n",
      "      and LayerNorm weights in AdamW optimizer\n",
      "  adam_beta1:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.9'\n",
      "    description: beta1 hyperparameter for the AdamW optimizer\n",
      "  adam_beta2:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.999'\n",
      "    description: beta2 hyperparameter for the AdamW optimizer\n",
      "  adam_epsilon:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: 1e-08\n",
      "    description: epsilon hyperparameter for the AdamW optimizer\n",
      "  gradient_accumulation_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: Number of updates steps to accumulate the gradients for, before performing\n",
      "      a backward/update pass\n",
      "  eval_accumulation_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '-1'\n",
      "    description: Number of predictions steps to accumulate before moving the tensors\n",
      "      to the CPU, will be passed as None if set to -1\n",
      "  lr_scheduler_type:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: linear\n",
      "    description: learning rate scheduler to use.\n",
      "    enum:\n",
      "    - linear\n",
      "    - cosine\n",
      "    - cosine_with_restarts\n",
      "    - polynomial\n",
      "    - constant\n",
      "    - constant_with_warmup\n",
      "  precision:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: '32'\n",
      "    description: Apply mixed precision training. This can reduce memory footprint\n",
      "      by performing operations in half-precision.\n",
      "    enum:\n",
      "    - '32'\n",
      "    - '16'\n",
      "  seed:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '42'\n",
      "    description: Random seed that will be set at the beginning of training\n",
      "  enable_full_determinism:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: Ensure reproducible behavior during distributed training. Check this\n",
      "      link https://pytorch.org/docs/stable/notes/randomness.html for more details.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  dataloader_num_workers:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '0'\n",
      "    description: Number of subprocesses to use for data loading. 0 means that the\n",
      "      data will be loaded in the main process.\n",
      "  ignore_mismatched_sizes:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: Not setting this flag will raise an error if some of the weights\n",
      "      from the checkpoint do not have the same size as the weights of the model.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  max_grad_norm:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '1.0'\n",
      "    description: Maximum gradient norm (for gradient clipping)\n",
      "  evaluation_strategy:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: epoch\n",
      "    description: The evaluation strategy to adopt during training. If set to \"steps\",\n",
      "      either the `evaluation_steps_interval` or `eval_steps` needs to be specified,\n",
      "      which helps to determine the step at which the model evaluation needs to be\n",
      "      computed else evaluation happens at end of each epoch.\n",
      "    enum:\n",
      "    - epoch\n",
      "    - steps\n",
      "    - disable\n",
      "  evaluation_steps_interval:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.0'\n",
      "    description: The evaluation steps in fraction of an epoch steps to adopt during\n",
      "      training. Overwrites eval_steps if not 0.\n",
      "  eval_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '500'\n",
      "    description: Number of update steps between two evals if evaluation_strategy='steps'\n",
      "  logging_strategy:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: steps\n",
      "    description: The logging strategy to adopt during training. If set to \"steps\",\n",
      "      the `logging_steps` will decide the frequency of logging else logging happens\n",
      "      at the end of epoch..\n",
      "    enum:\n",
      "    - epoch\n",
      "    - steps\n",
      "  logging_steps:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '10'\n",
      "    description: Number of update steps between two logs if logging_strategy='steps'\n",
      "  metric_for_best_model:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: loss\n",
      "    description: metric to use to compare two different model checkpoints\n",
      "    enum:\n",
      "    - loss\n",
      "    - f1\n",
      "    - exact\n",
      "  resume_from_checkpoint:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to \"true\", resumes the training from last saved checkpoint.\n",
      "      Along with loading the saved weights, saved optimizer, scheduler and random\n",
      "      states will be loaded if exist. The default value is \"false\"\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  save_total_limit:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '-1'\n",
      "    description: If a positive value is passed, it will limit the total number of\n",
      "      checkpoints saved. The value of -1 saves all the checkpoints, otherwise if the\n",
      "      number of checkpoints exceed the _save_total_limit_, the older checkpoints gets\n",
      "      deleted.\n",
      "  apply_early_stopping:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to \"true\", early stopping is enabled.\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  early_stopping_patience:\n",
      "    type: integer\n",
      "    optional: true\n",
      "    default: '1'\n",
      "    description: Stop training when the metric specified through _metric_for_best_model_\n",
      "      worsens for _early_stopping_patience_ evaluation calls.This value is only valid\n",
      "      if _apply_early_stopping_ is set to true.\n",
      "  early_stopping_threshold:\n",
      "    type: number\n",
      "    optional: true\n",
      "    default: '0.0'\n",
      "    description: Denotes how much the specified metric must improve to satisfy early\n",
      "      stopping conditions. This value is only valid if _apply_early_stopping_ is set\n",
      "      to true.\n",
      "  apply_deepspeed:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to true, will enable deepspeed for training\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  deepspeed_stage:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: '2'\n",
      "    description: This parameter configures which DEFAULT deepspeed config to be used\n",
      "      - stage2 or stage3. The default choice is stage2. Note that, this parameter\n",
      "      is ONLY applicable when user doesn't pass any config information via deepspeed\n",
      "      port.\n",
      "    enum:\n",
      "    - '2'\n",
      "    - '3'\n",
      "  apply_ort:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: 'false'\n",
      "    description: If set to true, will use the ONNXRunTime training\n",
      "    enum:\n",
      "    - 'true'\n",
      "    - 'false'\n",
      "  evaluation_config_params:\n",
      "    type: string\n",
      "    optional: true\n",
      "    description: Additional parameters as JSON serielized string\n",
      "  compute_model_import:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: serverless\n",
      "    description: compute to be used for model_import eg. provide 'FT-Cluster' if your\n",
      "      compute is named 'FT-Cluster'. Special characters like \\ and ' are invalid in\n",
      "      the parameter value. If compute cluster name is provided, instance_type field\n",
      "      will be ignored and the respective cluster will be used\n",
      "  compute_preprocess:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: serverless\n",
      "    description: compute to be used for preprocess eg. provide 'FT-Cluster' if your\n",
      "      compute is named 'FT-Cluster'. Special characters like \\ and ' are invalid in\n",
      "      the parameter value. If compute cluster name is provided, instance_type field\n",
      "      will be ignored and the respective cluster will be used\n",
      "  compute_finetune:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: serverless\n",
      "    description: compute to be used for finetune eg. provide 'FT-Cluster' if your\n",
      "      compute is named 'FT-Cluster'. Special characters like \\ and ' are invalid in\n",
      "      the parameter value. If compute cluster name is provided, instance_type field\n",
      "      will be ignored and the respective cluster will be used\n",
      "  compute_model_evaluation:\n",
      "    type: string\n",
      "    optional: true\n",
      "    default: serverless\n",
      "    description: compute to be used for model_eavaluation eg. provide 'FT-Cluster'\n",
      "      if your compute is named 'FT-Cluster'. Special characters like \\ and ' are invalid\n",
      "      in the parameter value. If compute cluster name is provided, instance_type field\n",
      "      will be ignored and the respective cluster will be used\n",
      "outputs:\n",
      "  pytorch_model_folder:\n",
      "    type: uri_folder\n",
      "    description: output folder containing _best_ model as defined by _metric_for_best_model_.\n",
      "      Along with the best model, output folder contains checkpoints saved after every\n",
      "      evaluation which is defined by the _evaluation_strategy_. Each checkpoint contains\n",
      "      the model weight(s), config, tokenizer, optimzer, scheduler and random number\n",
      "      states.\n",
      "  mlflow_model_folder:\n",
      "    type: mlflow_model\n",
      "    description: output folder containing _best_ finetuned model in mlflow format.\n",
      "creation_context:\n",
      "  created_at: '2025-09-16T03:21:29.016389+00:00'\n",
      "  created_by: Microsoft\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2025-09-16T03:21:29.016389+00:00'\n",
      "  last_modified_by: Microsoft\n",
      "  last_modified_by_type: User\n",
      "id: azureml://registries/azureml/components/chat_completion_pipeline/versions/0.0.80\n",
      "is_deterministic: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"chat_completion_pipeline\", label=\"latest\"\n",
    ")\n",
    "print(pipeline_component_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-sandbox-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
