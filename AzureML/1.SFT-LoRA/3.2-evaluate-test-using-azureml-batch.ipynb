{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bad6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global logging level\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Specifically reduce Azure-related logging in this notebook\n",
    "logging.getLogger(\"azure\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143eb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Azure ML workspace configuration from config.yml\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Azure ML workspace configuration\n",
    "subscription_id = config[\"subscription_id\"]\n",
    "resource_group = config[\"resource_group\"]\n",
    "workspace_name = config[\"workspace_name\"]\n",
    "\n",
    "# Finetuned model batch endpoint configuration\n",
    "batch_endpoint_name = config[\"batch_endpoint_name\"]\n",
    "batch_deployment_name = config[\"batch_deployment_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script cmd\n",
    "az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MLClient instance\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data\n",
    "with open(\"./data/test.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32291f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch input data for scoring\n",
    "output_file = \"batch_input.jsonl\"\n",
    "max_items = len(test_data)  # Use all items in the test set\n",
    "batch_data = []\n",
    "for i, item in enumerate(test_data[:max_items]):\n",
    "    question = item[\"question\"]\n",
    "    options = item[\"options\"]\n",
    "    answer_idx = item[\"answer_idx\"]\n",
    "    \n",
    "    # Format options as A. Option text...\n",
    "    formatted_options = \"\\n\".join([f\"{key}. {val}\" for key, val in sorted(options.items())])\n",
    "    \n",
    "    # Create request in the format expected by the model\n",
    "    request = {\n",
    "        \"id\": f\"item_{i}\",  # Unique identifier for tracking\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical expert. Read the following USMLE question and choose the best answer. Give me the answer as A/B/C/D/E.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question:\\n{question}\\n\\nOptions:\\n{formatted_options}\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.1,\n",
    "        \"ground_truth\": answer_idx  # For evaluation purposes\n",
    "    }\n",
    "    batch_data.append(request)\n",
    "\n",
    "# Save to JSONL format for batch processing\n",
    "with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "    for item in batch_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Created batch input file '{output_file}' with {len(batch_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c087cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit batch scoring job\n",
    "input_file = \"batch_input.jsonl\"\n",
    "try:\n",
    "    # Use direct file path approach\n",
    "    input_data = Input(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=f\"./{input_file}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Invoking batch endpoint...\")\n",
    "    job = ml_client.batch_endpoints.invoke(\n",
    "        endpoint_name=batch_endpoint_name,\n",
    "        input=input_data,\n",
    "        deployment_name=batch_deployment_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch job submitted successfully!\")\n",
    "    print(f\"Job name: {job.name}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Batch scoring job failed: {e}\")\n",
    "    print(f\"Error details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to resume a batch scoring job if it was interrupted\n",
    "# Note: This requires the job ID from a previous run\n",
    "# input_file = \"batch_input.jsonl\"\n",
    "# try:\n",
    "#     # Use direct file path approach\n",
    "#     input_data = Input(\n",
    "#         type=AssetTypes.URI_FILE,\n",
    "#         path=f\"./{input_file}\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Invoking batch endpoint...\")\n",
    "#     job = ml_client.batch_endpoints.invoke(\n",
    "#         endpoint_name=batch_endpoint_name,\n",
    "#         input=input_data,\n",
    "#         deployment_name=batch_deployment_name,\n",
    "#         resume_from=\"batchjob-29ca7dc5-aa57-4289-935e-f03b0428891c\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Batch job resubmitted successfully!\")\n",
    "#     print(f\"Job name: {job.name}\")\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Batch scoring job failed: {e}\")\n",
    "#     print(f\"Error details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the results\n",
    "ml_client.jobs.download(job.name, download_path=\"./batch_results\")\n",
    "print(\"Results downloaded to './batch_results'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuned model batch endpoint evaluation\n",
    "with open(\"./batch_results/batch_input_results.json\", \"r\", encoding='utf-8') as f:\n",
    "    batch_results = json.load(f)\n",
    "\n",
    "print(f\"Total number of predictions: {len(batch_results)}\")\n",
    "\n",
    "# Calculate accuracy and per-class metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = len(batch_results)\n",
    "\n",
    "# Store predictions and ground truths for metrics calculation\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for result in batch_results:\n",
    "    prediction = result[\"prediction\"]\n",
    "    ground_truth = result[\"ground_truth\"]\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    ground_truths.append(ground_truth)\n",
    "    \n",
    "    if prediction == ground_truth:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "# Calculate per-class precision, recall, and F1\n",
    "classes = sorted(set(ground_truths + predictions))\n",
    "precision_per_class = {}\n",
    "recall_per_class = {}\n",
    "f1_per_class = {}\n",
    "\n",
    "for cls in classes:\n",
    "    # True Positives, False Positives, False Negatives\n",
    "    tp = sum(1 for p, g in zip(predictions, ground_truths) if p == cls and g == cls)\n",
    "    fp = sum(1 for p, g in zip(predictions, ground_truths) if p == cls and g != cls)\n",
    "    fn = sum(1 for p, g in zip(predictions, ground_truths) if p != cls and g == cls)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    precision_per_class[cls] = precision\n",
    "    recall_per_class[cls] = recall\n",
    "    f1_per_class[cls] = f1\n",
    "\n",
    "# Calculate macro-averaged metrics\n",
    "macro_precision = sum(precision_per_class.values()) / len(precision_per_class) if precision_per_class else 0\n",
    "macro_recall = sum(recall_per_class.values()) / len(recall_per_class) if recall_per_class else 0\n",
    "macro_f1 = sum(f1_per_class.values()) / len(f1_per_class) if f1_per_class else 0\n",
    "\n",
    "print(f\"\\nBatch Inference Results:\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Total predictions: {total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.2%}\")\n",
    "print(f\"Recall: {macro_recall:.2%}\")\n",
    "print(f\"F1-Score: {macro_f1:.2%}\")\n",
    "\n",
    "print(f\"\\nPer-class Metrics:\")\n",
    "for cls in classes:\n",
    "    print(f\"Class {cls}:\")\n",
    "    print(f\"  Precision: {precision_per_class[cls]:.2%}\")\n",
    "    print(f\"  Recall: {recall_per_class[cls]:.2%}\")\n",
    "    print(f\"  F1-Score: {f1_per_class[cls]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a4003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-sandbox-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
